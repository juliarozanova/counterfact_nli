{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_handle = 'ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli'\n",
    "# model_handle = 'chromeNLP/textattack_bert_base_MNLI_fixed'\n",
    "# model_handle = 'facebook/bart-large-mnli'\n",
    "# model_name = 'bert-base-uncased-snli-help'\n",
    "# model_handle = '../models/bert-base-uncased-snli-help/'\n",
    "\n",
    "model_handle = './models/infobert-checkpoint/'\n",
    "model_name = 'infobert'\n",
    "\n",
    "dataset_name = 'snli'\n",
    "# split = 'validation_matched'\n",
    "# split = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models/infobert-checkpoint/ were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_handle, resume_download=True)\n",
    "label2id = model.config.label2id\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'infobert': {\n",
    "        'entailment': 2, \n",
    "        'neutral': 1, \n",
    "        'contradiction': 0,\n",
    "    },\n",
    "\n",
    "    'bert-base-uncased-snli': {\n",
    "        'entailment': 1,\n",
    "        'neutral': 2, \n",
    "        'contradiction': 0\n",
    "    },\n",
    "\n",
    "    'bert-base-uncased-snli-help': {\n",
    "        'entailment': 1,\n",
    "        'neutral': 2, \n",
    "        'contradiction': 2\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset snli (/home/julia/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown split \"validation_matched\". Should be one of ['test', 'train', 'validation'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m builder \u001b[39m=\u001b[39m load_dataset_builder(dataset_name)\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(dataset_name, split\u001b[39m=\u001b[39;49msplit)\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m x :  x[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m!=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m builder\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/load.py:1769\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1767\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1768\u001b[0m )\n\u001b[0;32m-> 1769\u001b[0m ds \u001b[39m=\u001b[39m builder_instance\u001b[39m.\u001b[39;49mas_dataset(split\u001b[39m=\u001b[39;49msplit, ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications, in_memory\u001b[39m=\u001b[39;49mkeep_in_memory)\n\u001b[1;32m   1770\u001b[0m \u001b[39m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[39mif\u001b[39;00m task \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/builder.py:1066\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     split \u001b[39m=\u001b[39m {s: s \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits}\n\u001b[1;32m   1065\u001b[0m \u001b[39m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m datasets \u001b[39m=\u001b[39m map_nested(\n\u001b[1;32m   1067\u001b[0m     partial(\n\u001b[1;32m   1068\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_single_dataset,\n\u001b[1;32m   1069\u001b[0m         run_post_process\u001b[39m=\u001b[39;49mrun_post_process,\n\u001b[1;32m   1070\u001b[0m         ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[1;32m   1071\u001b[0m         in_memory\u001b[39m=\u001b[39;49min_memory,\n\u001b[1;32m   1072\u001b[0m     ),\n\u001b[1;32m   1073\u001b[0m     split,\n\u001b[1;32m   1074\u001b[0m     map_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1075\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m logging\u001b[39m.\u001b[39;49mis_progress_bar_enabled(),\n\u001b[1;32m   1076\u001b[0m )\n\u001b[1;32m   1077\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(datasets, \u001b[39mdict\u001b[39m):\n\u001b[1;32m   1078\u001b[0m     datasets \u001b[39m=\u001b[39m DatasetDict(datasets)\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/utils/py_utils.py:436\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39m# Singleton\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n\u001b[0;32m--> 436\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    438\u001b[0m disable_tqdm \u001b[39m=\u001b[39m disable_tqdm \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m    439\u001b[0m iterable \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data_struct\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m data_struct\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/builder.py:1097\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     split \u001b[39m=\u001b[39m Split(split)\n\u001b[1;32m   1096\u001b[0m \u001b[39m# Build base dataset\u001b[39;00m\n\u001b[0;32m-> 1097\u001b[0m ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_as_dataset(\n\u001b[1;32m   1098\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m   1099\u001b[0m     in_memory\u001b[39m=\u001b[39;49min_memory,\n\u001b[1;32m   1100\u001b[0m )\n\u001b[1;32m   1101\u001b[0m \u001b[39mif\u001b[39;00m run_post_process:\n\u001b[1;32m   1102\u001b[0m     \u001b[39mfor\u001b[39;00m resource_file_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_processing_resources(split)\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/builder.py:1168\u001b[0m, in \u001b[0;36mDatasetBuilder._as_dataset\u001b[0;34m(self, split, in_memory)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[39m\"\"\"Constructs a `Dataset`.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \n\u001b[1;32m   1154\u001b[0m \u001b[39mThis is the internal implementation to overwrite called when user calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39m    `Dataset`\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m cache_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs\u001b[39m.\u001b[39m_strip_protocol(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir)\n\u001b[0;32m-> 1168\u001b[0m dataset_kwargs \u001b[39m=\u001b[39m ArrowReader(cache_dir, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo)\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m   1169\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m   1170\u001b[0m     instructions\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m   1171\u001b[0m     split_infos\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49msplits\u001b[39m.\u001b[39;49mvalues(),\n\u001b[1;32m   1172\u001b[0m     in_memory\u001b[39m=\u001b[39;49min_memory,\n\u001b[1;32m   1173\u001b[0m )\n\u001b[1;32m   1174\u001b[0m fingerprint \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dataset_fingerprint(split)\n\u001b[1;32m   1175\u001b[0m \u001b[39mreturn\u001b[39;00m Dataset(fingerprint\u001b[39m=\u001b[39mfingerprint, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs)\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/arrow_reader.py:235\u001b[0m, in \u001b[0;36mBaseReader.read\u001b[0;34m(self, name, instructions, split_infos, in_memory)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\n\u001b[1;32m    215\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    216\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m     in_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    220\u001b[0m ):\n\u001b[1;32m    221\u001b[0m     \u001b[39m\"\"\"Returns Dataset instance(s).\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m         kwargs to build a single Dataset instance.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     files \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_file_instructions(name, instructions, split_infos)\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m files:\n\u001b[1;32m    237\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInstruction \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00minstructions\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m corresponds to no data!\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/arrow_reader.py:208\u001b[0m, in \u001b[0;36mBaseReader.get_file_instructions\u001b[0;34m(self, name, instruction, split_infos)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_file_instructions\u001b[39m(\u001b[39mself\u001b[39m, name, instruction, split_infos):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     file_instructions \u001b[39m=\u001b[39m make_file_instructions(\n\u001b[1;32m    209\u001b[0m         name, split_infos, instruction, filetype_suffix\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filetype_suffix, prefix_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    211\u001b[0m     files \u001b[39m=\u001b[39m file_instructions\u001b[39m.\u001b[39mfile_instructions\n\u001b[1;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;00m files\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/arrow_reader.py:125\u001b[0m, in \u001b[0;36mmake_file_instructions\u001b[0;34m(name, split_infos, instruction, filetype_suffix, prefix_path)\u001b[0m\n\u001b[1;32m    123\u001b[0m     instruction \u001b[39m=\u001b[39m ReadInstruction\u001b[39m.\u001b[39mfrom_spec(instruction)\n\u001b[1;32m    124\u001b[0m \u001b[39m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m absolute_instructions \u001b[39m=\u001b[39m instruction\u001b[39m.\u001b[39;49mto_absolute(name2len)\n\u001b[1;32m    127\u001b[0m \u001b[39m# For each split, return the files instruction (skip/take)\u001b[39;00m\n\u001b[1;32m    128\u001b[0m file_instructions \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/arrow_reader.py:648\u001b[0m, in \u001b[0;36mReadInstruction.to_absolute\u001b[0;34m(self, name2len)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_absolute\u001b[39m(\u001b[39mself\u001b[39m, name2len):\n\u001b[1;32m    637\u001b[0m     \u001b[39m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \n\u001b[1;32m    639\u001b[0m \u001b[39m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[39m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 648\u001b[0m     \u001b[39mreturn\u001b[39;00m [_rel_to_abs_instr(rel_instr, name2len) \u001b[39mfor\u001b[39;00m rel_instr \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/arrow_reader.py:648\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_absolute\u001b[39m(\u001b[39mself\u001b[39m, name2len):\n\u001b[1;32m    637\u001b[0m     \u001b[39m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \n\u001b[1;32m    639\u001b[0m \u001b[39m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[39m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 648\u001b[0m     \u001b[39mreturn\u001b[39;00m [_rel_to_abs_instr(rel_instr, name2len) \u001b[39mfor\u001b[39;00m rel_instr \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/datasets/arrow_reader.py:460\u001b[0m, in \u001b[0;36m_rel_to_abs_instr\u001b[0;34m(rel_instr, name2len)\u001b[0m\n\u001b[1;32m    458\u001b[0m split \u001b[39m=\u001b[39m rel_instr\u001b[39m.\u001b[39msplitname\n\u001b[1;32m    459\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m name2len:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnknown split \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(name2len)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    461\u001b[0m num_examples \u001b[39m=\u001b[39m name2len[split]\n\u001b[1;32m    462\u001b[0m from_ \u001b[39m=\u001b[39m rel_instr\u001b[39m.\u001b[39mfrom_\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown split \"validation_matched\". Should be one of ['test', 'train', 'validation']."
     ]
    }
   ],
   "source": [
    "builder = load_dataset_builder(dataset_name)\n",
    "dataset = load_dataset(dataset_name, split=split).filter(lambda x :  x['label']!=-1)\n",
    "builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/julia/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-2b17587843f58083.arrow\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# guess_label2id = {\n",
    "#     'entailment': 0,\n",
    "#     'neutral': 2, \n",
    "#     'contradiction': 1\n",
    "# }\n",
    "\n",
    "# mnli same as dataset label2id\n",
    "# guess_label2id = {\n",
    "#     'entailment': 0,\n",
    "#     'neutral': 1, \n",
    "#     'contradiction': 2\n",
    "# }\n",
    "\n",
    "# best for roberta-large-mnli\n",
    "# guess_label2id = {\n",
    "#     'entailment': 2,\n",
    "#     'neutral': 1, \n",
    "#     'contradiction': 0\n",
    "# }\n",
    "\n",
    "#guess for for bert-base-uncased-snli-help\n",
    "guess_label2id = {\n",
    "    'entailment': 1,\n",
    "    'neutral': 2, \n",
    "    'contradiction': 2\n",
    "}\n",
    "\n",
    "# best for bert-base-uncased-snli\n",
    "# guess_label2id = {\n",
    "#     'entailment': 1,\n",
    "#     'neutral': 2, \n",
    "#     'contradiction': 0\n",
    "# }\n",
    "\n",
    "dataset = dataset.align_labels_with_mapping(label2id[model_name], 'label')\n",
    "# dataset = dataset.align_labels_with_mapping(label2id, 'label')\n",
    "dataset.set_format(type=\"torch\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       2\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "9810    1\n",
       "9811    0\n",
       "9812    2\n",
       "9813    2\n",
       "9814    0\n",
       "Name: y_true, Length: 9815, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "results = pd.DataFrame(results)\n",
    "results[\"y_true\"] = dataset['label'].to('cpu')\n",
    "results[\"y_true\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_handle)\n",
    "except OSError:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('textattack/bert-base-uncased-snli')\n",
    "max_length=256\n",
    "def encode(examples):\n",
    "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.85ba/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.09ba/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(encode, batched=True)\n",
    "dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"premise\", \"hypothesis\",  \"labels\"], device='cuda')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The new rights are nice enough</td>\n",
       "      <td>Everyone really likes the newest benefits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This site includes a list of all award winners...</td>\n",
       "      <td>The Government Executive articles housed on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>uh i don't know i i have mixed emotions about ...</td>\n",
       "      <td>I like him for the most part, but would still ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>yeah i i think my favorite restaurant is alway...</td>\n",
       "      <td>My favorite restaurants are always at least a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i don't know um do you do a lot of camping</td>\n",
       "      <td>I know exactly.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_true                                            premise  \\\n",
       "0       1                     The new rights are nice enough   \n",
       "1       0  This site includes a list of all award winners...   \n",
       "2       2  uh i don't know i i have mixed emotions about ...   \n",
       "3       0  yeah i i think my favorite restaurant is alway...   \n",
       "4       0         i don't know um do you do a lot of camping   \n",
       "\n",
       "                                          hypothesis  \n",
       "0         Everyone really likes the newest benefits   \n",
       "1  The Government Executive articles housed on th...  \n",
       "2  I like him for the most part, but would still ...  \n",
       "3  My favorite restaurants are always at least a ...  \n",
       "4                                    I know exactly.  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset\n",
    "df = df[:]\n",
    "results['premise'] = df['premise']\n",
    "results['hypothesis'] = df['hypothesis']\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [04:07<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    for inputs in tqdm(dataloader):\n",
    "        batch_outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        batch_logits = batch_outputs['logits'].to('cpu')\n",
    "        batch_predictions = np.argmax(batch_logits, axis=1)\n",
    "        y_pred += batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"y_pred\"] = y_pred\n",
    "results[\"y_pred\"] = results[\"y_pred\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.loc[results.y_true!=results.y_pred].value_counts()\n",
    "# results.loc[results.y_true==results.y_pred].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred\n",
    "#2112\n",
    "#y_true\n",
    "#1021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9110545084055017"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(results[\"y_true\"], results[\"y_pred\"])\n",
    "# f1_score(results[\"y_true\"], results[\"y_pred\"], average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9110545084055017"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(results['y_pred'], results['y_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3025\n",
       "1     141\n",
       "2      47\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"y_pred\"].loc[results.y_true==0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2769\n",
       "0     177\n",
       "2     177\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"y_pred\"].loc[results.y_true==1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3148\n",
       "1     280\n",
       "0      51\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"y_pred\"].loc[results.y_true==2].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| model | accuracy on snli| accuracy on mnli validation_matched|\n",
    "| --- | --- | --- |\n",
    "|ynie | 91.8464| --- |\n",
    "| bert-base-uncased-snli-help | 73.381 | 61.82 |\n",
    "| chromeNLP | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5631747dc3ad6eb60a3da5f4cdcebe149d51d3449295229bf66075ef01b5a217"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
